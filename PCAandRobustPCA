{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"},{"sourceId":9118157,"sourceType":"datasetVersion","datasetId":5504002},{"sourceId":9122164,"sourceType":"datasetVersion","datasetId":5506718}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vsevolodrakita/pca-and-robust-pca-theory-and-examples?scriptVersionId=191537035\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# PCA and Robust PCA: Theory and Examples","metadata":{}},{"cell_type":"markdown","source":"## Introduction\nIn this notebook, we will discuss Principal Component Analysis (PCA) and Robust PCA. In section 1, we briefly discuss the theory behind PCA. In section 2, we show some practical applications of PCA, and how to implement it using numpy. In section 3, we discuss Robust PCA, a more advanced technique.\n\nThis notebook presupposes knowledge of basic linear algebra concepts, such as eigenvalues and eigenvectors, projections and norms.\n","metadata":{}},{"cell_type":"markdown","source":"## The Theory of PCA\n### Motivation\nPrincipal component analysis is an algorithm for linear dimensionality reduction. Suppose we have $m$ data points $x_1,x_2,...,x_m$ with $x_i \\in \\mathbb{R}^d$. Denote by $X\\in \\mathbb{R}^{m\\times d}$ the matrix whose rows are $x_1,...,x_m$. If $m$ and $d$ are large, $X$ may be very inconvinient to work with - for instance, to just store the data we need to store $m*d$ numbers. Thus, we want to reduce the dimensionality of the data, i.e. we want a pair of functions $f:\\mathbb{R}^d \\rightarrow \\mathbb{R}^l$ (The *encoder*) and $g:\\mathbb{R}^l \\rightarrow \\mathbb{R}^d$ (the *decoder*), where $l<<d$, such that $g(f(x_i))$ is *close* to $x_i$ in some sense. If we have such functions, instead of storing the $m \\times d$ matrix $X$, we can store the (smaller) $m \\times l$ matrix whose rows are $f(x_1),...,f(x_m)$.\n\nThe simplest functions from $\\mathbb{R}^d$ to $\\mathbb{R}^l$ are linear functions, so we will look for linear $f,g$. This will give us the PCA algorithm. In this case, $g\\circ f$ is a linear function from $\\mathbb{R}^d$ to a subspace $U$ of $\\mathbb{R}^d$. We will find $g\\circ f$ and $U$ directly.\n\n### Details\nLet $x_1,x_2,...,x_m$ with $x_i \\in \\mathbb{R}^d$, and fix $l<d$. We assume that the mean of $\\{x_i\\}$ is zero, i.e. $\\frac{1}{m}\\sum_{i=1}^m x_i=0$. This does not limit the generality, as if we are given a set $\\{x_1,x_2,...,x_m\\}$, we can denote $\\mu=\\frac{1}{m}\\sum_{i=1}^m x_i$ and consider the set $\\{x_1-\\mu,...,x_m-\\mu\\}$.\n\nWe are looking for a linear subspace $U\\subseteq \\mathbb{R}^d$ such that $dim(U)=l$ and $\\{x_1',...x_m'\\}$ such that $x_i'\\in U$ and the *average loss* $\\frac{1}{m}\\sum_{i=1}^m ||x_i-x'_i||_2^2$ is minimized. If we have $U$, we know that $U$ has an ortonormal basis $b_1,...,b_l$, and the $x_i'$ that minimize $\\sum_{i=1}^m ||x_i-x'_i||_2^2$ are the projections of the $x_i$s on to $U$, i.e. $x_i'=BB^tx_i=\\sum_{j=1}^l<x_i,bj>b_j$, where $B$ is the matrix whose columns are the $b_i$.\n\nThus, the only problem is to find the subspace $U$, or equivalently to find the basis elements $b_1,...,b_l$. For fixed basis elements $b_1,...,b_l$, we can find vectors $b_{l+1},...,b_{d}$ such that $b_1,...,b_d$ forms an orthonormal basis of $\\mathbb{R}^d$, and we know that for all $1\\leq i \\leq m$ we have $x_i=\\sum_{j=1}^d <x_i,b_j>b_j=\\sum_{j=1}^d b_j^tx_ib_j$.\n\nWe know further (since the $\\{b_j\\}$ form an orthonormal set) that \n$$x_i-x_i'=\\sum_{j=1}^d b_j^tx_ib_j -\\sum_{j=1}^l b_j^tx_ib_j=\\sum_{j=l+1}^db_j^tx_ib_j\n$$\n\nFor fixed $U$, denote $J_U=\\frac{1}{m}\\sum_{i=1}^m ||x_i-x'_i||_2^2$. Combining the last two equations, and using the ortogonality of the $\\{b_j\\}$ we have:\n$$\nJ_U=\\frac{1}{m}\\sum_{i=1}^m ||x_i-x'_i||_2^2=\\frac{1}{m}\\sum_{i=1}^m||\\sum_{j=l+1}^db_j^tx_ib_j||_2^2=\\frac{1}{m}\\sum_{i=1}^m\\sum_{j=l+1}^d(b_j^tx_i)^2=\\sum_{i=1}^m\\sum_{j=l+1}^db_j^t\\frac{1}{m}x_ix_i^tb_j\n$$\nChanging the order of summation we have\n$$\nJ_U=\\sum_{j=l+1}^d b_j^t \\frac{1}{m}\\sum_{i=1}^m[x_ix_i^t]b_j\n$$\nDenote $S=\\frac{1}{m}\\sum_{i=1}^mx_ix_i^t$, and note that this is the covariance matrix of the data $x_1,x_2,...,x_m$. We finally have:\n$$\nJ_U=\\sum_{j=l+1}^d b_j^tSb_j\n$$\nNow we want to select a subspace $U$ that minimizes this quantity. How can we do that? Recall that a covariance matrix is positive semidefinite and diagonalizable, and hence the matrix $S$ has $d$ (not necessarily distinct) eigenvalues $\\lambda_1\\geq\\lambda_2\\geq...\\geq\\lambda_d\\geq 0$. Thus, if we choose $\\{b_1,...,b_d\\}$ such that $b_j$ is an eigenvector of $\\lambda_j$, we get $J_U=\\sum_{j=l+1}^d \\lambda_j$. It is an easy exersise to show that for every other orthonormal basis, $J_U$ is some linear combination of eigenvalues that can not be smaller than $J_U=\\sum_{j=l+1}^d \\lambda_j$.\n\nNote that $J_U$ tells us  how much information is lost in going from the original data to the projection in $U$: The projection on to $U$ retains $0\\leq \\frac{\\sum_{j=1}^l \\lambda_j}{\\sum_{j=1}^d \\lambda_j}\\leq 1$ of the information in the original data.\n\nTo conclude, we have shown that given a data set $\\{x_1,x_2,...,x_m\\}$, we can find a subspace $U$ with basis $b_1,...,b_l$ that minimizes $\\frac{1}{m}\\sum_{i=1}^m ||x_i-x'_i||_2^2$ (where $x_i'$ is the projection of $x_i$ onto $U$) by taking the $l$ largest eigenvectors of the data covariance matrix $S=\\frac{1}{m}\\sum_{i=1}^mx_ix_i^t$. Those eigenvectors are called the *principal components*.\n\n### Remark\nThis, in my opinion, is the most straightforward way to derive PCA, since it only requiers using linear algebra. There are, however, (at least...) two other ways to derive PCA: using a maximum variance argument (this is the most common way I saw PCA derived online) and using a probabilistic argument. Both of these derivations are interesting, and you should explore them if you have the time! The maximum variance argument in particular shows that PCA finds the \"directions of maximum variance\", i.e. the directions in which the data varies the most.","metadata":{}},{"cell_type":"markdown","source":"## Practical Examples\nAfter looking at the theory of PCA, we will now look at how we can use PCA in practice, how to implement PCA and what we can do with it.\n\nFor our examples, we will use the famous MNIST digit data set from the [Digit Recognizer competition](https://www.kaggle.com/competitions/digit-recognizer). Every data point consists of a $28 \\times 28$ pixple picture of a handwritten digit from 0 to 9, given as a vector of length 784, along  with a label indicating which digit it is.","metadata":{}},{"cell_type":"markdown","source":"First, we load some libraries and load the data. There are 10 types of digits, which would be a bit cluttered, so we will only use a few digits.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:26.23754Z","iopub.execute_input":"2024-08-07T13:46:26.237908Z","iopub.status.idle":"2024-08-07T13:46:29.387381Z","shell.execute_reply.started":"2024-08-07T13:46:26.237868Z","shell.execute_reply":"2024-08-07T13:46:29.386305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\neights=df[df[\"label\"]==8]\neights.pop(\"label\")\nones=df[df[\"label\"]==1]\nones.pop(\"label\")\nsevens=df[df[\"label\"]==7]\nsevens.pop(\"label\")\nnines=df[df[\"label\"]==9]\n_=nines.pop(\"label\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:29.389806Z","iopub.execute_input":"2024-08-07T13:46:29.39049Z","iopub.status.idle":"2024-08-07T13:46:33.044694Z","shell.execute_reply.started":"2024-08-07T13:46:29.390447Z","shell.execute_reply":"2024-08-07T13:46:33.043593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at some of the pictures of the digit 9:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 5\nfor i in range(1, columns*rows +1):\n    img = np.array(nines)[[i],:].reshape(28,28)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:33.045712Z","iopub.execute_input":"2024-08-07T13:46:33.046018Z","iopub.status.idle":"2024-08-07T13:46:35.329639Z","shell.execute_reply.started":"2024-08-07T13:46:33.045992Z","shell.execute_reply":"2024-08-07T13:46:35.328489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by only examining the digits 8 and 1. We have 8747 data points, with each data point a vector of length 784 (so $n=8747$ and $d=784$).","metadata":{}},{"cell_type":"code","source":"temp1=np.array(eights)\ntemp2=np.array(ones)\ndata=np.vstack([temp1,temp2])\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:35.332691Z","iopub.execute_input":"2024-08-07T13:46:35.333226Z","iopub.status.idle":"2024-08-07T13:46:35.405814Z","shell.execute_reply.started":"2024-08-07T13:46:35.333187Z","shell.execute_reply":"2024-08-07T13:46:35.404782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The easiest way to use PCA in python is using the PCA module in sklearn. We will only take 2 principal components ($l=2$), and all we have to do is fit PCA on the data.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:35.406989Z","iopub.execute_input":"2024-08-07T13:46:35.407323Z","iopub.status.idle":"2024-08-07T13:46:35.685659Z","shell.execute_reply.started":"2024-08-07T13:46:35.407295Z","shell.execute_reply":"2024-08-07T13:46:35.684343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here `pca.explained_variance_ratio_` gives us $[\\frac{\\lambda_1}{\\sum_{j=1}^{784} \\lambda_j},\\frac{\\lambda_2}{\\sum_{j=1}^{784} \\lambda_j}]$, so we can see that the first principal component accounts for a bit more than 20% of the variance in the data, and the second principal component accounts for a bit more than 11%.\n\n`pca.singular_values_` gives us the eigenvalues $\\lambda_1, \\lambda_2$ themselves.\n\nLets look at the transformed data on the $\\mathbb{R}^2$ plane:","metadata":{}},{"cell_type":"code","source":"reduced_data=pca.transform(data)\npx.scatter(x=reduced_data[:,0], y=reduced_data[:,1], color=([\"8\"]*len(eights)+[\"1\"]*len(ones)))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:35.687932Z","iopub.execute_input":"2024-08-07T13:46:35.689995Z","iopub.status.idle":"2024-08-07T13:46:37.507142Z","shell.execute_reply.started":"2024-08-07T13:46:35.689947Z","shell.execute_reply":"2024-08-07T13:46:37.506094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we apply `pca.transform` to the data matrix, we get the projection of the original data (which lives in $\\mathbb{R}^{784}$) on to the first two principal components, isomorphic to $\\mathbb{R}^2$. We can see that the digits 1 and 8 both form nice, distinctive groups. If we wanted to make a classifier that distinguishes between the two groups, we would achive very good results even with simple models like support vector machines or k nearest neighbours.\n\nLets now implement PCA oureselves. ","metadata":{}},{"cell_type":"code","source":"temp1=np.array(eights)\ntemp2=np.array(ones)\ndata=np.vstack([temp1,temp2])\ncentered_data=(data-data.mean(axis=0))\ncovariance_matrix = np.cov(centered_data, ddof = 1, rowvar = False)\n#Equivalent to\n#covariance_matrix =(standardized_data.T@standardized_data)/(len(temp1)+len(temp2)-1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:37.508509Z","iopub.execute_input":"2024-08-07T13:46:37.50891Z","iopub.status.idle":"2024-08-07T13:46:37.787133Z","shell.execute_reply.started":"2024-08-07T13:46:37.508864Z","shell.execute_reply":"2024-08-07T13:46:37.785843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first center our data in the line `centered_data=(data-data.mean(axis=0))`, and then compute the covariance matrix.\nSome times when doing PCA, we would also want to normalize the data, i.e. devide each column by a constant so that the variance of each feature is 1. We would want to do that if the data in each column is measured in different units, or is on different scales - in this case, if we don't normalize, one scale may dominate the others. In our case, the data in every column measures the opacity of a particular pixel, so they are all on the same scale, and hence normalizing is unnecessary.\n\nNow, we compute the eigenvalues and eigenvectors of the covariance matrix. Since it is a real symmetrical matrix, we can use the numpy `eigh` function.\n","metadata":{}},{"cell_type":"code","source":"eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix) #np.allclose(covariance_matrix@eigenvectors[:,-1],eigenvalues[-1]*eigenvectors[:,-1])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:37.792315Z","iopub.execute_input":"2024-08-07T13:46:37.795548Z","iopub.status.idle":"2024-08-07T13:46:37.911049Z","shell.execute_reply.started":"2024-08-07T13:46:37.795495Z","shell.execute_reply":"2024-08-07T13:46:37.909699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: in practic, calculating the eigenvalues and eigenvectors of a large matrix is computationally very taxing. Our images are relatively small, so we can do it, but if they were larger we would need to use some numerical techniques to approximate them.\n\nIf we did everything correctly, the explained variance and graph of the projections should be simillar to what we got using the sklearn PCA module.","metadata":{}},{"cell_type":"code","source":"explained_variance=eigenvalues/np.sum(eigenvalues)\nprint(explained_variance[-1])\nprint(explained_variance[-2])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:37.916562Z","iopub.execute_input":"2024-08-07T13:46:37.919227Z","iopub.status.idle":"2024-08-07T13:46:37.92808Z","shell.execute_reply.started":"2024-08-07T13:46:37.919179Z","shell.execute_reply":"2024-08-07T13:46:37.926794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ev1=eigenvectors[:,-1]\nev2=eigenvectors[:,-2]\ntrans_mat=np.vstack([ev1,ev2]).T\nreduced=centered_data@trans_mat\npx.scatter(x=reduced[:,0], y=reduced[:,1], color=([\"8\"]*len(eights)+[\"1\"]*len(ones)))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:37.933556Z","iopub.execute_input":"2024-08-07T13:46:37.934259Z","iopub.status.idle":"2024-08-07T13:46:38.063172Z","shell.execute_reply.started":"2024-08-07T13:46:37.934216Z","shell.execute_reply":"2024-08-07T13:46:38.062082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's do the same process with the 7's and 1's:","metadata":{}},{"cell_type":"code","source":"temp1=np.array(sevens)\ntemp2=np.array(ones)\ndata=np.vstack([temp1,temp2])\nstandardized_data=(data-data.mean(axis=0))\ncovariance_matrix = np.cov(standardized_data, ddof = 1, rowvar = False)\neigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\nev1=eigenvectors[:,-1]\nev2=eigenvectors[:,-2]\ntrans_mat=np.vstack([ev1,ev2]).T\nreduced=standardized_data@trans_mat\npx.scatter(x=reduced[:,0], y=reduced[:,1], color=([\"7\"]*len(sevens)+[\"1\"]*len(ones)))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:38.064849Z","iopub.execute_input":"2024-08-07T13:46:38.065801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And with the 7's, 1's, 8's, and 9's:","metadata":{}},{"cell_type":"code","source":"temp1=np.array(sevens)\ntemp2=np.array(ones)\ntemp3=np.array(eights)\ntemp4=np.array(nines)\ndata=np.vstack([temp1,temp2,temp3,temp4])\nstandardized_data=(data-data.mean(axis=0))\ncovariance_matrix = np.cov(standardized_data, ddof = 1, rowvar = False)\neigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\nev1=eigenvectors[:,-1]\nev2=eigenvectors[:,-2]\ntrans_mat=np.vstack([ev1,ev2]).T\nreduced=standardized_data@trans_mat\npx.scatter(x=reduced[:,0], y=reduced[:,1], color=([\"7\"]*len(sevens)+[\"1\"]*len(ones))+[\"8\"]*len(eights)+[\"9\"]*len(nines))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:38.520817Z","iopub.execute_input":"2024-08-07T13:46:38.521444Z","iopub.status.idle":"2024-08-07T13:46:39.219002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even with four classes, we still get decent seperation.\n\nWhat else can we do with PCA? Well, we can produce entirely new data! We can do this by generating a Gaussian noise vector with the size of the number of our principal components with mean 0 and variance equal to the variance of the transformed data set along each component, and transforming it back to the original dimension.\n\nHere is an example with 1-4 principal components:","metadata":{}},{"cell_type":"code","source":"pcas=[]\nstds=[]\nfor i in range(4):\n    pcas.append(PCA(n_components=i+1))\n    pcas[-1].fit(nines)\n    stds.append(pcas[-1].transform(nines).std(axis=0))\n\nfig = plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 4\nfor i in range(1, columns*rows +1):\n    x=np.random.randn((i-1)//4+1)*stds[(i-1)//4]\n    img = pcas[(i-1)//4].inverse_transform(x).reshape(28,28)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:39.220304Z","iopub.execute_input":"2024-08-07T13:46:39.221157Z","iopub.status.idle":"2024-08-07T13:46:42.172338Z","shell.execute_reply.started":"2024-08-07T13:46:39.221117Z","shell.execute_reply":"2024-08-07T13:46:42.171256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we mentioned in the theory section, PCA can also be used to compress information. Consider, for instance, the following image of a face:","metadata":{}},{"cell_type":"code","source":"img=Image.open(\"/kaggle/input/aface1/bc2369c525020fe9948fd120219b97b2.jpg\")\nimg=img.convert('L') #Convert the image to greyscale\nprint(np.array(img).shape)\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:42.173645Z","iopub.execute_input":"2024-08-07T13:46:42.17397Z","iopub.status.idle":"2024-08-07T13:46:42.531022Z","shell.execute_reply.started":"2024-08-07T13:46:42.173942Z","shell.execute_reply":"2024-08-07T13:46:42.529987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use PCA to compress and then decompress the image, losing as little information as possible:","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=20)\ncompressed_img=pca.fit_transform(img)\nprint(compressed_img.shape)\ndecompressed_img=pca.inverse_transform(compressed_img)\nplt.imshow(decompressed_img)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:42.532596Z","iopub.execute_input":"2024-08-07T13:46:42.532946Z","iopub.status.idle":"2024-08-07T13:46:43.205346Z","shell.execute_reply.started":"2024-08-07T13:46:42.532917Z","shell.execute_reply":"2024-08-07T13:46:43.20431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we used 20 components, so the compressed image was 1/3 the size of the original. The more components you use, the larger the compressed image will be, but the better the restored image:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 4\nj=1\nfor i in range(1,61,5):\n    pca = PCA(n_components=i)\n    compressed_img=pca.fit_transform(img)\n    decompressed_img=pca.inverse_transform(compressed_img)\n    fig.add_subplot(rows, columns, j)\n    plt.imshow(decompressed_img)\n    plt.title(f\"n_components={i}\")\n    j+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:43.206766Z","iopub.execute_input":"2024-08-07T13:46:43.207605Z","iopub.status.idle":"2024-08-07T13:46:46.899132Z","shell.execute_reply.started":"2024-08-07T13:46:43.207566Z","shell.execute_reply":"2024-08-07T13:46:46.897083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Robust PCA\n### Motivation\nPCA is very sensitive to noise: For example, consider the following image:","metadata":{}},{"cell_type":"code","source":"img=Image.open('/kaggle/input/aface1/Noise_salt_and_pepper.png')\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:46.901643Z","iopub.execute_input":"2024-08-07T13:46:46.902201Z","iopub.status.idle":"2024-08-07T13:46:47.225344Z","shell.execute_reply.started":"2024-08-07T13:46:46.902157Z","shell.execute_reply":"2024-08-07T13:46:47.224115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the image is noisy. If we apply PCA to it, we get the following","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nfig.add_subplot(1, 2, 1)\nim=np.array(img)\npca = PCA(n_components=10)\nim2=pca.fit_transform(im)\nim3=pca.inverse_transform(im2)\nplt.imshow(im3)\nplt.title(f\"PCA with 10 components\")\nfig.add_subplot(1, 2, 2)\npca = PCA(n_components=25)\nim2=pca.fit_transform(im)\nim3=pca.inverse_transform(im2)\nplt.imshow(im3)\nplt.title(f\"PCA with 25 components\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:47.226774Z","iopub.execute_input":"2024-08-07T13:46:47.227202Z","iopub.status.idle":"2024-08-07T13:46:48.164969Z","shell.execute_reply.started":"2024-08-07T13:46:47.227164Z","shell.execute_reply":"2024-08-07T13:46:48.16397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ideally, we would want to remove the noise before applying PCA. This is the motivation for *Robust PCA*, described in the paper [Robust Principal Component Analysis?](https://arxiv.org/abs/0912.3599)\n\n### Theory\nLet $X\\in \\mathbb{R}^{d\\times m}$ be the data matrix. We want to write $X=L+S$ where $L$ is a low rank matrix and $S$ is a sparse matrix. That way, all the noise will be in $S$, and $L$ will contain the information we want to work with. Formally, we want to solve the following optimization problem:\n$$\n\\min_{L,S} rank(L)+||S||_0\n$$\nwhere $||S||_0$ is the *$l_0$ pseudo norm*, i.e. the number of non-zero entries in $S$. This is a hard problem to solve, so instead we relax it and solve the problem\n$$\n\\min_{L,S} ||L||_* + \\lambda||S||_1 s.t. X=L+S\n$$\nwhere $\\lambda = (\\sqrt{max(d,m)})^{-1}$ and $||L||_*=trace(\\sqrt{L^tL})$ is the *nuclear norm* of $L$ (note that the rank of $L$ is equal to the number of non zero eigenvalues of $L^tL$, so minimizeing $||L||_*$ should force $L$ to be low rank, and minimizing $\\lambda||S||_1$ should force $S$ to be sparse). This problem is known as *Principal Component Pursuit*.\n\n\nTo solve the Principal component pursuit, we use the [Augmented Lagrange multiplier](https://en.wikipedia.org/wiki/Augmented_Lagrangian_method) algorithm: define \n$$\nl(L,S,Y)=||L||_* + \\lambda||S||_1 +<Y,X-L-S>+\\frac{\\mu}{2} ||X-L-S||^2_F\n$$\nIn general, to solve such problems we procced iteratively: we find $L_k,S_k$ such that $l(L_k,S_k,Y_k)$ is minimized, update $Y_{k+1}=Y_k+\\mu(X-L_k-S_k)$ and iterate.\n\nIn this case, however, it makes more sense to use the *alternating directions method*: First minimize $l(L,S,Y)$ with respet to $L$ (fixing $S$), then minimize $l(L,S,Y)$ with respet to $S$ (fixing $L$), and finally update the Lagraunge multiplier $Y$ based on $L,S$. The reason we do this is because the problems $\\min_L l(L,S,Y)$ and $min_S l(L,S,Y)$ have simple solutions:\n\nLet $Sh_\\tau: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the *shrinkage operator*, defined by $Sh_\\tau(x)=sign(x)\\max(|x|-\\tau,0)$. If $L,Y$ are fixed, it can be shown that the $S$ that minimizes $l(L,S,Y)$ is given by $Sh_{\\lambda\\mu}(X-L+\\mu^{-1}Y)$ ($Sh_{\\lambda\\mu}$ here is applied to the matrix element-wise).\n\nFor a matrix $M$, let $D_\\tau(M)=USh_\\tau(\\Sigma)V^t$ be the *Singular value tresholding operator*, where $U\\Sigma V^t$ is a singular decomposition of $M$. Then for fixed $S,Y$, the matrix $L$ that minimizes $l(L,S,Y)$ is given by $D_\\mu(X-S+\\mu^{-1}Y)$.\n\nLets implement the algorithm.\n\n### Implementation\nFirst, we implement the shrinkage and singular value tresholding operators:","metadata":{}},{"cell_type":"code","source":"def shrinkage_operator(X,tau):\n    temp = np.abs(X)-tau\n    return np.sign(X) * np.maximum(temp,np.zeros_like(temp))\n\ndef svto(X,tau):\n    U,S,VT = np.linalg.svd(X,full_matrices=0)\n    return U @ np.diag(shrinkage_operator(S,tau)) @ VT","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:46:48.166271Z","iopub.execute_input":"2024-08-07T13:46:48.166583Z","iopub.status.idle":"2024-08-07T13:46:48.172127Z","shell.execute_reply.started":"2024-08-07T13:46:48.166555Z","shell.execute_reply":"2024-08-07T13:46:48.171136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we compute $L_k,S_k$ iteratively.","metadata":{}},{"cell_type":"code","source":"def robust_pca(X):\n    r,m = X.shape\n    mu = r*m/(np.sum(np.abs(X.reshape(-1))))\n    lambd = 1/np.sqrt(np.maximum(r,m))\n    S = np.zeros_like(X)\n    Y = np.zeros_like(X)\n    L = np.zeros_like(X)\n    count = 0\n    for _ in range(1000):\n        L = svto(X-S+(1/mu)*Y,1/mu)\n        S = shrinkage_operator(X-L+(1/mu)*Y,lambd/mu)\n        Y = Y + mu*(X-L-S)\n        count += 1\n    return L,S,np.linalg.norm(X-L-S)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:48:09.247448Z","iopub.execute_input":"2024-08-07T13:48:09.247817Z","iopub.status.idle":"2024-08-07T13:48:09.255155Z","shell.execute_reply.started":"2024-08-07T13:48:09.247789Z","shell.execute_reply":"2024-08-07T13:48:09.25412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example\nNow, lets clean our image using robust PCA:","metadata":{}},{"cell_type":"code","source":"L,S,eps=robust_pca(im)\nprint(eps)\nfig = plt.figure(figsize=(8, 8))\nfig.add_subplot(1, 2, 1)\nplt.imshow(L)\nfig.add_subplot(1, 2, 2)\nplt.imshow(S)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:48:11.548542Z","iopub.execute_input":"2024-08-07T13:48:11.548933Z","iopub.status.idle":"2024-08-07T13:49:00.568735Z","shell.execute_reply.started":"2024-08-07T13:48:11.5489Z","shell.execute_reply":"2024-08-07T13:49:00.56766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The resultin image is much cleaner. Applying PCA to the clean image, we again get much cleaner results:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nfig.add_subplot(1, 2, 1)\nim=np.array(img)\npca = PCA(n_components=10)\nim2=pca.fit_transform(L)\nim3=pca.inverse_transform(im2)\nplt.imshow(im3)\nfig.add_subplot(1, 2, 2)\npca = PCA(n_components=25)\nim2=pca.fit_transform(L)\nim3=pca.inverse_transform(im2)\nplt.imshow(im3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:49:11.68644Z","iopub.execute_input":"2024-08-07T13:49:11.686823Z","iopub.status.idle":"2024-08-07T13:49:12.395806Z","shell.execute_reply.started":"2024-08-07T13:49:11.686793Z","shell.execute_reply":"2024-08-07T13:49:12.394688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conculsion\nThanks for reading!","metadata":{}}]}